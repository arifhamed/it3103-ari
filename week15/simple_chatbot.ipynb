{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "simple_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week15/simple_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBcdHO6HqntD"
      },
      "source": [
        "# Simple Chatbot\n",
        "\n",
        "In this final part, we will take the models that we have trained and use them to recognize an intent and the entities and building simple responses to that. \n",
        "\n",
        "Before starting, click on the Colab's Runtime > Manage Sessions menu. Click the \"TERMINATE OTHER SESSIONS\" button.  \n",
        "\n",
        "Then, run the following cells to download the models (after training them to our intent and token classification tasks) and install the necessary libraries. \n",
        "\n",
        "The reason we are doing this is because downloading the models that you have trained from Colab is VERY slow. So we've already saved a copy of our own trained models and uploaded it to a public server on Amazon for download.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSsEBxfeqnTc"
      },
      "source": [
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/intent_model.zip\n",
        "!wget https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/pretrained-models/token_model.zip\n",
        "!unzip intent_model.zip\n",
        "!unzip token_model.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI9SzGqc5rid"
      },
      "source": [
        "Next, run the following to install a specific version of the HuggingFace Transformers library.\n",
        "\n",
        "Our model was trained against this version of the library, so it is advisable to use the same version for prediction / inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ZHduZsD9FM"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIfHlpofrhsw"
      },
      "source": [
        "## Section 1 - Inferring Intent\n",
        "\n",
        "In this section, we declare the codes to infer intent based on a single line of input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7arnC-lxUx0"
      },
      "source": [
        "# Import the necessary libraries\n",
        "#\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    TFAutoModelForSequenceClassification\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Create the DistilBERT tokenizer\n",
        "#\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "\n",
        "# Create a list of unique labels that we will recognize.\n",
        "#\n",
        "sentence_labels = [\n",
        "              \"others\",\n",
        "              \"atis_abbreviation\",\n",
        "              \"atis_aircraft\",\n",
        "              \"atis_airfare\",\n",
        "              \"atis_airline\",\n",
        "              \"atis_flight\",\n",
        "              \"atis_flight_time\",\n",
        "              \"atis_greeting\",\n",
        "              \"atis_ground_service\",\n",
        "              \"atis_quantity\",\n",
        "              \"atis_yes\",\n",
        "              \"atis_no\"]\n",
        "\n",
        "# Define a function to perform inference on a single input text.\n",
        "# \n",
        "def infer_intent(text, model, tokenizer):\n",
        "    # Passes the text into the tokenizer\n",
        "    #\n",
        "    input = tokenizer(text, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "    # Sends the result from the tokenizer into our classification model\n",
        "    #\n",
        "    output = model(input)\n",
        "    pred_label = np.argmax(tf.nn.softmax(output.logits, axis=-1))\n",
        "\n",
        "    # Return the result to the caller\n",
        "    #\n",
        "    return sentence_labels[pred_label]\n",
        "\n",
        "\n",
        "# Load the saved model file\n",
        "#\n",
        "intent_model = TFAutoModelForSequenceClassification.from_pretrained('intent_model')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfalBIty8HE9"
      },
      "source": [
        "Run the following cell to test the codes that infers the intent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGku6HIC7tus"
      },
      "source": [
        "infer_intent(\"How much is the ticket to fly to New York\", intent_model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPh__bBSrsXg"
      },
      "source": [
        "## Section 2 - Inferring Entity\n",
        "\n",
        "In this section, we declare the codes to infer entities for each individual word in a line of text. The entities are then constructed and returned to the caller.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuqsojUoPKLl"
      },
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForTokenClassification\n",
        ")\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctC7Sb93PhNl"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRFGoqUEvayO"
      },
      "source": [
        "# Define a list of unique labels that we will recognized\n",
        "#\n",
        "token_labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "# Define the function to infer the individual tokens\n",
        "#\n",
        "def infer_tokens(text, model, tokenizer):\n",
        "    # here we assume the text has not been splitted into individual words\n",
        "    text = text.split()\n",
        "    \n",
        "    encodings = tokenizer(\n",
        "        [text],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors='tf')\n",
        "    \n",
        "    logits = model(encodings)[0] # assume only a single prediction\n",
        "    preds = np.argmax(logits, axis=-1)[0]\n",
        "\n",
        "    # as the prediction is on individual tokens, including subtokens, \n",
        "    # we need to group subtokens belonging to the same word together\n",
        "    # again, we use the word_ids to help us here\n",
        "    previous_word_idx = None\n",
        "    word_ids = encodings[0].word_ids\n",
        "    labels = []\n",
        "    for i, word_idx in enumerate(word_ids):\n",
        "        # we check if the word_id different from previous one, then it is a new word\n",
        "        # we also need to check if the word_id is not None so that we won't include it\n",
        "        if word_idx != previous_word_idx and word_idx != None:\n",
        "            labels.append(label_names[preds[i]])\n",
        "        # update the previous_word_idx to current word_id\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return text, labels\n",
        "\n",
        "\n",
        "# Define the function to combine individual tokens into a dictionary\n",
        "#\n",
        "def infer_combined_tokens(text, token_model, tokenizer):\n",
        "    result = {\n",
        "        \"PER\" : [],\n",
        "        \"LOC\" : [],\n",
        "        \"ORG\" : [],\n",
        "        \"MISC\" : []\n",
        "    }\n",
        "\n",
        "    result_tokens, result_texts = infer_tokens(text, token_model, tokenizer)\n",
        "\n",
        "    current_token_label = \"\"\n",
        "    current_result_index = -1;    \n",
        "\n",
        "    for i in range(len(result_tokens)):\n",
        "        #print(result_tokens[i])\n",
        "        if token_labels[result_tokens[i]].startswith(\"B-\"):\n",
        "            current_token_label = token_labels[result_tokens[i]].replace(\"B-\", \"\")\n",
        "            result[current_token_label].append(result_texts[i])\n",
        "            current_result_index = len(result[current_token_label]) - 1\n",
        "        elif token_labels[result_tokens[i]].startswith(\"I-\"):\n",
        "            result[current_token_label][current_result_index] += \" \" + result_texts[i]\n",
        "    \n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFlcl1NOzqcQ"
      },
      "source": [
        "token_model = TFAutoModelForTokenClassification.from_pretrained('token_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2YiSzKZPZlg"
      },
      "source": [
        "text, tokens = infer_tokens(\"How much is the ticket to fly to New York\", token_model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siH0F4r7QLvY"
      },
      "source": [
        "print(tokens)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjnIpnGJ8AYv"
      },
      "source": [
        "Run the following cell to test the codes that extracts and combines all the entities for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK-8pQV574uI"
      },
      "source": [
        "infer_combined_tokens(\"Peter Leong and John Lim of Aims are going to fly to New York\", \n",
        "                      token_model, \n",
        "                      tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fBzlRb7vbXe"
      },
      "source": [
        "## Section 3 - Implementing Logic for Our Chatbot\n",
        "\n",
        "In this section, let's implement some very basic logic for our chatbot. We will make use of the two functions that we wrote above.\n",
        "\n",
        "You can implement some simple logic that looks like the following:\n",
        "\n",
        "```\n",
        "        if (intent == \"atis_flight\" or intent == \"atis_airline\") and len(tokens[\"LOC\"]):\n",
        "            print (\"Can I confirmed if you just asked about flying to \" + tokens[\"LOC\"][0])\n",
        "        elif intent == \"atis_yes\":\n",
        "            print (\"Great, then let's me book the ticket for you\")\n",
        "        elif intent == \"atis_no\":\n",
        "            print (\"Oh I am sorry what did I get wrong?\")\n",
        "        elif intent == \"atis_greeting\":\n",
        "            print (\"Hi, how are you?\")            \n",
        "        else:\n",
        "            print (\"I don't quite know how to respond to \" + intent + \" yet.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBZ-U3dQLj3H"
      },
      "source": [
        "def chatbot():\n",
        "    print (\"Chatbot Started. Press 'Q'+Enter to quit.\")\n",
        "\n",
        "    while (True):\n",
        "        input_text = input()\n",
        "        if input_text == \"Q\" or input_text == \"\":\n",
        "            break\n",
        "\n",
        "        intent = infer_intent(intent_model, input_text)\n",
        "        tokens = infer_combined_tokens(token_model, input_text)\n",
        "\n",
        "        # TODO: \n",
        "        # Write you own logic to conduct a conversation with the user\n",
        "        # about buying tickets and flying somewhere.\n",
        "        #...#\n",
        "        if (intent == \"atis_flight\" or intent == \"atis_airline\") and len(tokens[\"LOC\"]):\n",
        "            print (\"Can I confirmed if you just asked about flying to \" + tokens[\"LOC\"][0])\n",
        "        elif intent == \"atis_yes\":\n",
        "            print (\"Great, then let me book the ticket for you\")\n",
        "        elif intent == \"atis_no\":\n",
        "            print (\"Oh I am sorry what did I get wrong?\")\n",
        "        elif intent == \"atis_greeting\":\n",
        "            print (\"Hi, how are you?\")            \n",
        "        else:\n",
        "            print (\"I don't quite know how to respond to \" + intent + \" yet.\")\n",
        "\n",
        "    print (\"Good bye!\")\n",
        "\n",
        "chatbot()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}