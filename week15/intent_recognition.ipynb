{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "intent_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week15/intent_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTTt2on1Kdr-"
      },
      "source": [
        "# Intent Recognition\n",
        "\n",
        "In this practical, we will learn how to apply the HuggingFace Transformers library to our own Intent Recognition task for our chatbot.\n",
        "\n",
        "####**NOTE: Be sure to set your runtime to a GPU instance!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prPHVBopKx1O"
      },
      "source": [
        "## Install the Hugging Face Transformers Library\n",
        "\n",
        "Run the following cell below to install the transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXufUo5E0mfK"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eyUK96X91ud"
      },
      "source": [
        "## Getting the data and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX2EMQRpl5kA"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_url = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/datasets/airchat_intents.csv'\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbViUOJL_B5m"
      },
      "source": [
        "We noticed that there are two columns 'Label' and 'Text'. Let's just examine what are the different labels we have and how many samples we have for each labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flTF4DB-_Qee"
      },
      "source": [
        "df['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCOUBlEr_cJ1"
      },
      "source": [
        "We can see that some labels have very few sample such as 'atis_meal', 'atis_airline#atis_flight_no', 'atis_cheapest', and so on. With so few samples, our model will have difficulty in learning any meaningful pattern from it. We will group these labels (with few samples) into a new label called 'others'.  \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr6YP12kwOyD"
      },
      "source": [
        "### Re-define our Classification Labels\n",
        "\n",
        "Here we define the labels we are interested in classifying based on the original labels, and also we added a new label called 'Others'.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdoYqzUQvnRt"
      },
      "source": [
        "# Create a list of unique labels that we will recognize.\n",
        "#\n",
        "sentence_labels = [\n",
        "              \"others\",\n",
        "              \"atis_abbreviation\",\n",
        "              \"atis_aircraft\",\n",
        "              \"atis_airfare\",\n",
        "              \"atis_airline\",\n",
        "              \"atis_flight\",\n",
        "              \"atis_flight_time\",\n",
        "              \"atis_greeting\",\n",
        "              \"atis_ground_service\",\n",
        "              \"atis_quantity\",\n",
        "              \"atis_yes\",\n",
        "              \"atis_no\"]\n",
        "\n",
        "# This creates a reverse mapping dictionary of \"label\" -> index.\n",
        "# \n",
        "sentence_labels_id_by_label = dict((t, i) for i, t in enumerate(sentence_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51O9M5_4Aao3"
      },
      "source": [
        "Now we will map the previous labels to the few ones we specified in the cell above. We will also convert the text labels into numeric labels (e.g. others->0, atis_abbreviation->1, etc). We can use the `map()` function in dataframe to help us do that. We define a lambda function that do the mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmQkgwklnQ41"
      },
      "source": [
        "df['Label'] = df['Label'].map(lambda label: \n",
        "                              sentence_labels_id_by_label[label] \n",
        "                              if label in sentence_labels_id_by_label \n",
        "                              else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7KmBHB3A-DP"
      },
      "source": [
        "# examine a few random samples \n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXn8KmHAxc1l"
      },
      "source": [
        "### Split Our Data\n",
        "\n",
        "We will now separate the texts and labels and call them all_texts and all_labels and we will split the dataset into training and validation set. We do a stratified split to ensure we have equal representation of different labels in both train and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLTOzdnKB2Us"
      },
      "source": [
        "all_texts = df['Text']\n",
        "all_labels = df['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVyqTUPKxdOR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(all_texts, \n",
        "                                                                    all_labels, \n",
        "                                                                    test_size=0.2, \n",
        "                                                                    stratify=all_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgKGYupJo59k"
      },
      "source": [
        "train_labels.value_counts()/len(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UpZf9s2o_Xx"
      },
      "source": [
        "val_labels.value_counts()/len(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "milh-Q2-yZBe"
      },
      "source": [
        "### Tokenize the text \n",
        "\n",
        "Before we can use the text for classification, we need to tokenize them. We will use Tokenizer of the pretrained model 'distilbert-base-uncased' as we will be fine-tunining on a pretrained model 'distilbert-base-uncased'. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentence_labels)"
      ],
      "metadata": {
        "id": "5l_bBQ7dBc9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_8gIcR8yZJl"
      },
      "source": [
        "## before we can feed the texts to tokenizer, we need to convert our texts into list of text string instead of \n",
        "## panda Series. We can do this by using to_list(). \n",
        "\n",
        "train_texts = train_texts.to_list()\n",
        "train_labels = train_labels.to_list()\n",
        "val_texts = val_texts.to_list()\n",
        "val_labels = val_labels.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEDfpTUZqGBK"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFpKNCpLp5n2"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsO-EGFIDs7Z"
      },
      "source": [
        "Once we have the encodings, we will go ahead and create a tensorflow dataset, ready to be used to train our model. Since the HuggingFace pretrained model (the tensorflow version) is a Keras model, it can consume the tf.data dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH9Bv9SKp9e2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        ")).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmAO-RI8y8fX"
      },
      "source": [
        "## Train Your Sentence Classification Model\n",
        "\n",
        "Run the following cell to download the \"distilbert-base-uncased\" and perform fine-tuning training using the dataset that we have above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",num_labels=len(sentence_labels))"
      ],
      "metadata": {
        "id": "2zgiouIb-Kpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in previous lab, we start with a smaller learning rate 5e-5 (0.00005) and slowly reduce the learning rate over the course of training."
      ],
      "metadata": {
        "id": "O5giXIw5Jnuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Since our dataset is already batched, we can simply take the len.\n",
        "num_train_steps = len(train_dataset) * num_epochs\n",
        "\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "qWIOLntZ-l82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "opt = Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "Q39NhOmK-w9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W60-n-I0TNX"
      },
      "source": [
        "### Evaluating the Model\n",
        "\n",
        "Run the following code to evaluate our model with entire validation data set.\n",
        "\n",
        "We also print out the classification report to see how the model performs for each label. Note that those with smaller number of samples typically have lower F1-score.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbLVVeCFuNku"
      },
      "source": [
        "output = model.predict(val_dataset, batch_size=1)\n",
        "pred_probs = tf.nn.softmax(output.logits, axis=-1)\n",
        "preds = tf.argmax(pred_probs, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels = []\n",
        "for _, labels in val_dataset.as_numpy_iterator():\n",
        "    val_labels.extend(labels)"
      ],
      "metadata": {
        "id": "x73ytg7dIGvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUt2HCZOufZN"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(val_labels, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT_ucCQv0FSG"
      },
      "source": [
        "### Saving the Model\n",
        "\n",
        "When you training has completed, run the following cell to save your model.\n",
        "\n",
        "Remember to download the model from Google Colab if you want to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eV7zUC-A1Bu"
      },
      "source": [
        "# Save the model\n",
        "\n",
        "model.save_pretrained(\"intent_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9T_A7_F3OrR"
      },
      "source": [
        "## Putting Our Model to the Test\n",
        "\n",
        "Run the following cell to create the necessary classes and functions to load our model and perform inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dq45oOFH9Sc"
      },
      "source": [
        "# Import the necessary libraries\n",
        "#\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForSequenceClassification\n",
        ")\n",
        "\n",
        "# Create the DistilBERT tokenizer\n",
        "#\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Define a function to perform inference on a single input text.\n",
        "# \n",
        "def infer_intent(model, text):\n",
        "    # Passes the text into the tokenizer\n",
        "    #\n",
        "    input = tokenizer(text, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "    \n",
        "    # Sends the result from the tokenizer into our classification model\n",
        "    #\n",
        "    output = model(input)\n",
        "\n",
        "    # Extract the output logits and convert to softmax \n",
        "    # Find the classification index with the highest value.\n",
        "    #  \n",
        "    pred_label = tf.argmax(tf.nn.softmax(output.logits, axis=-1), axis=-1)\n",
        "\n",
        "    return pred_label\n",
        "\n",
        "# Create a list of unique labels that we will recognize.\n",
        "# Obviously this has to match what we trained our model with\n",
        "# earlier.\n",
        "#\n",
        "sentence_labels = [\n",
        "              \"others\",\n",
        "              \"atis_abbreviation\",\n",
        "              \"atis_aircraft\",\n",
        "              \"atis_airfare\",\n",
        "              \"atis_airline\",\n",
        "              \"atis_flight\",\n",
        "              \"atis_flight_time\",\n",
        "              \"atis_greeting\",\n",
        "              \"atis_ground_service\",\n",
        "              \"atis_quantity\",\n",
        "              \"atis_yes\",\n",
        "              \"atis_no\"]\n",
        "\n",
        "# Load the saved model file\n",
        "#\n",
        "intent_model = TFAutoModelForSequenceClassification.from_pretrained(\"intent_model\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3fouh-hMkEm"
      },
      "source": [
        "text = input()\n",
        "\n",
        "print (sentence_labels[infer_intent(intent_model, text)[0]])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}