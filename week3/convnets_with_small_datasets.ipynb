{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "convnets-with-small-datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week3/convnets_with_small_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RZIgnFFEv5t"
      },
      "source": [
        "# Lab Exercise: Image Classification using Convolutional Neural Network \n",
        "\n",
        "In this practical, we will see how we can use a Convolutional Neural Network to classify cat and dog images.\n",
        "\n",
        "We will train the network using relatively little data (about 2000 images) which is a common real problem with a lot of deep learning projects where data is hard to come by. We have learnt in the lecture how we can solve the small data problem with some common techniques like data augmentation and transfer learning. We will examine how to use data augmentation in this lab and in the next lab, we will learn to use transfer learning.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1bPIkzHsK-y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQgbpqHw-vXy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydQUHIqs-vX6"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "The cats vs. dogs dataset is available at Kaggle.com as part of a computer vision \n",
        "competition in late 2013. You can download the [original dataset](\n",
        "https://www.kaggle.com/c/dogs-vs-cats/data) from Kaggle (you will need to create a Kaggle account if you don't already have one)\n",
        "\n",
        "The pictures are medium-resolution color JPEGs and are of various sizes and shapes that look like this:\n",
        "\n",
        "<img src='https://nypai.s3.ap-southeast-1.amazonaws.com/it3103/resources/cats_vs_dogs_samples.jpg' height='300'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d76mWqf7-vX6"
      },
      "source": [
        "This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). For the purpose of demonstrating challenges of training with small data set and also to have an opportunity to see the effects of using data augmentation technique, we will use a smaller subset (2000 train images and 1000 validation images) which you can download from [here](https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_filtered.zip). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSQZGCRtReJY"
      },
      "source": [
        "In the codes below, we use the keras ``get_file()`` utility to download and unzip the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grI88iK2-vX6"
      },
      "source": [
        "import os, shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JRhg0UBDRT1"
      },
      "source": [
        "dataset_URL = 'https://nypai.s3.ap-southeast-1.amazonaws.com/datasets/cats_and_dogs_filtered.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=dataset_URL, extract=True, cache_dir='.')\n",
        "print(path_to_zip)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phy4xWMT-vX7"
      },
      "source": [
        "# Directories for our training,\n",
        "# train and validation splits\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYJ9QpHC-vX9"
      },
      "source": [
        "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvvIwWbb-vX_"
      },
      "source": [
        "print('total training cat images:', len(os.listdir(train_cats_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVaFjM9s-vX_"
      },
      "source": [
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVjMyPmZ-vYA"
      },
      "source": [
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rXZ-dlU-vYA"
      },
      "source": [
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chQJRgxL-vYB"
      },
      "source": [
        "\n",
        "So we indeed have 2000 training images, 1000 validation images and 1000 test images. In each split, there is the same number of \n",
        "samples from each class: this is a balanced binary classification problem, which means that classification accuracy will be an appropriate \n",
        "measure of success."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMXLfrh6-vYB"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "Our convnet will be a stack of alternate `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
        "\n",
        "**Exercise 1**: \n",
        "\n",
        "Write the codes to implement the following: \n",
        "\n",
        "- Input layer should be of shape (150,150,3)\n",
        "- The hidden layers consist of the following Conv2D/MaxPooling2D blocks:\n",
        "  - Block 1: Conv layer with 32 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - Block 2: Conv layer with 64 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - Block 3 and 4: Conv layer with 128 filters with filter size of 3x3, followed by MaxPooling layer\n",
        "  - A Layer to convert 2D to 1D\n",
        "- A Dense Layer with 512 neurons\n",
        "- Output layer using Dense Layer\n",
        "\n",
        "Use RELU as activation functions for all hidden layers. \n",
        "\n",
        "What activation function should you use for the output layer?\n",
        "\n",
        "<br/>\n",
        "\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-6zYJGn-vYB"
      },
      "source": [
        "### TODO: Write the code to build the model and compile the model \n",
        "\n",
        "model = None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EN4zoVi-vYC"
      },
      "source": [
        "Let's print the model summary to show the shape and paramater numbers for each layer. Your output should look something like this: \n",
        "\n",
        "<img src=\"https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/resources/expected_convnet_summary.png\" width=400 />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FJD8b_2-vYC"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqBfX0po-vYC"
      },
      "source": [
        "**Exercise 2**: \n",
        "\n",
        "Compile your model with the appropriate optimizer and loss function. We will use RMSProp with learning rate of 1e-4 and monitor the 'accuracy' metrics. What should we use for the loss function? \n",
        "\n",
        "Complete the code below. \n",
        "\n",
        "<br/>\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
        "              metrics=['acc'])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOdPRQ8j-vYC"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "### TODO: Complete the code below ####\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We2Khkc-vYC"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Image data should be formatted into appropriately pre-processed floating point tensors before being fed into our \n",
        "network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:\n",
        "\n",
        "* Read the picture files.\n",
        "* Decode the JPEG content to RGB grids of pixels.\n",
        "* Resize the image into same size (in our case, we will use 150 by 150)\n",
        "* Convert these into floating point tensors.\n",
        "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n",
        "\n",
        "It may seem a bit daunting, but tf.keras provides the class `ImageDataGenerator` which allows to \n",
        "quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx7_Oujp-vYD"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzFJ1ptz-vYD"
      },
      "source": [
        "Let's take a look at the output of one of these generators: it yields batches of 150x150 RGB images (shape `(20, 150, 150, 3)`) and binary \n",
        "labels (shape `(20,)`). 20 is the number of samples in each batch (the batch size). Note that the generator yields these batches \n",
        "indefinitely: it just loops endlessly over the images present in the target folder. For this reason, we need to `break` the iteration loop \n",
        "at some point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQJxIZGy-vYD"
      },
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOx2NhAYQ0XF"
      },
      "source": [
        "How do we know what label is assigned to each of the class? We can use class_indices of the ImageGenerator to show the mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fifOVDCQwva"
      },
      "source": [
        "validation_generator.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tedmUKwPDHFe"
      },
      "source": [
        "## Visualization using Tensorboard\n",
        "\n",
        "Let's define a utility function to create a Tensorboard callback function that can be used by the model training later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI2leTe5C2oR"
      },
      "source": [
        "def create_tb_callback(): \n",
        "\n",
        "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "\n",
        "    def get_run_logdir():    # use a new directory for each run\n",
        "\t    import time\n",
        "\t    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "\t    return os.path.join(root_logdir, run_id)\n",
        "\n",
        "    run_logdir = get_run_logdir()\n",
        "\n",
        "    tb_callback = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "    return tb_callback\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4YMOgW8REAx"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lt_DbKQ-vYD"
      },
      "source": [
        "Let's fit our model to the data using the generator. It expects as first argument a Python generator that will yield batches of inputs and targets indefinitely, like ours does. \n",
        "Because the data is being generated endlessly, the generator needs to know how many samples to draw from the generator before \n",
        "declaring an epoch over. This is the role of the `steps_per_epoch` argument: after having drawn `steps_per_epoch` batches from the \n",
        "generator, i.e. after having run for `steps_per_epoch` gradient descent steps, the fitting process will go to the next epoch. In our case, \n",
        "batches are 20-sample large, so it will take 100 batches until we see our target of 2000 samples.\n",
        "\n",
        "When using `fit`, one may pass a `validation_data`.Importantly, this argument is \n",
        "allowed to be a data generator itself, but it could be a tuple of Numpy arrays as well. If you pass a generator as `validation_data`, then \n",
        "this generator is expected to yield batches of validation data endlessly, and thus you should also specify the `validation_steps` argument, \n",
        "which tells the process how many batches to draw from the validation generator for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwyFdTu8-vYD"
      },
      "source": [
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_acc', patience=10, verbose=0,\n",
        "    mode='auto', restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      callbacks=[create_tb_callback(), earlystop_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R9qafGC9U9e"
      },
      "source": [
        "Let's evaluate the model on the validation data. We should see the model performance same as the best validation accuracy during training, as we specify ``restore_best_weights`` in our EarlyStopping Callback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej84-o7v-vYE"
      },
      "source": [
        "It is good practice to always save your models after training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cEi0nA79o6e"
      },
      "source": [
        "model.evaluate(validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btxvjweb-vYE"
      },
      "source": [
        "model.save('cats_and_dogs_small_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS2NA64l-vYE"
      },
      "source": [
        "Let's visualize our training accuracy and loss using Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eje5UEye-vYE"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEI1hpgL67Ts"
      },
      "source": [
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOZTbSxL-vYE"
      },
      "source": [
        "These plots are characteristic of overfitting. Our training accuracy increases linearly over time, until it reaches nearly 100%, while our \n",
        "validation accuracy stalls at 72-74%. Our validation loss reaches its minimum after only five epochs then stalls, while the training loss \n",
        "keeps decreasing linearly until it reaches nearly 0.\n",
        "\n",
        "Because we only have relatively few training samples (2000), overfitting is going to be our number one concern. There are a \n",
        "number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We are now going to \n",
        "use one, specific to computer vision, and used almost universally when processing images with deep learning models: *data \n",
        "augmentation*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9YQpQ8-vYE"
      },
      "source": [
        "## Using data augmentation\n",
        "\n",
        "Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. \n",
        "Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data \n",
        "augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number \n",
        "of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same \n",
        "picture twice. This helps the model get exposed to more aspects of the data and generalize better.\n",
        "\n",
        "In Keras, this can be done by configuring a number of random transformations to be performed on the images read by our `ImageDataGenerator` \n",
        "instance. Let's get started with an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbAfiBdu-vYE"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qiDkZc-vYF"
      },
      "source": [
        "These are just a few of the options available (for more, see the Keras documentation). Let's quickly go over what we just wrote:\n",
        "\n",
        "* `rotation_range` is a value in degrees (0-180), a range within which to randomly rotate pictures.\n",
        "* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures \n",
        "vertically or horizontally.\n",
        "* `shear_range` is for randomly applying shearing transformations.\n",
        "* `zoom_range` is for randomly zooming inside pictures.\n",
        "* `horizontal_flip` is for randomly flipping half of the images horizontally -- relevant when there are no assumptions of horizontal \n",
        "asymmetry (e.g. real-world pictures).\n",
        "* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
        "\n",
        "Let's take a look at our augmented images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxLPiFmw-vYF"
      },
      "source": [
        "# This is module with image preprocessing utilities\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "\n",
        "# We pick one image to \"augment\"\n",
        "img_path = fnames[3]\n",
        "\n",
        "# Read the image and resize it\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "\n",
        "# Convert it to a Numpy array with shape (150, 150, 3)\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# Reshape it to (1, 150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)\n",
        "\n",
        "# The .flow() command below generates batches of randomly transformed images.\n",
        "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    plt.figure(i)\n",
        "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
        "    i += 1\n",
        "    if i % 4 == 0:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I6r5H9B-vYF"
      },
      "source": [
        "If we train a new network using this data augmentation configuration, our network will never see twice the same input. However, the inputs \n",
        "that it sees are still heavily intercorrelated, since they come from a small number of original images -- we cannot produce new information, \n",
        "we can only remix existing information. As such, this might not be quite enough to completely get rid of overfitting. To further fight \n",
        "overfitting, we will also add a Dropout layer to our model, right before the densely-connected classifier:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gngudl4W-vYF"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFPI_eAP-vYF"
      },
      "source": [
        "Let's train our network using data augmentation and dropout. We also need to train for more epochs, so that our network has better chance of seeing all the original images (since now we cannot guarantee that for each epoch, our original image is chosen at least once, instead, the ImageDataGenerator may choose randomly transformed image instead)\n",
        "\n",
        "**Exercise 3:**\n",
        "\n",
        "Create ImageDataGenerator for both training and validation data with batch size of 50. Apply the following transformation: \n",
        "- zoom_range (0.2)\n",
        "- shear_range (0.2)\n",
        "- rotation_range (40)\n",
        "- brightness_range (0.2,1.0)\n",
        "- width_shift_range=0.2\n",
        "- height_shift_range=0.2\n",
        "- horizontal_flip\n",
        "\n",
        "DO NOT forget to normalize your pixel values to between (0,1). \n",
        "\n",
        "Fit your model by specifying appropriate number of steps to match your batch size. Specify both EarlyStopping and Tensorboard callbacks.\n",
        "\n",
        "<br/>\n",
        "<details>\n",
        "<summary>Click here for answer</summary>\n",
        "\n",
        "```\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.2,1.0])\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=50,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=50,\n",
        "        class_mode='binary')\n",
        "\n",
        "model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=40,\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=20, callbacks=[earlystop_callback, create_tb_callback()])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iXF1RZ5-vYF"
      },
      "source": [
        "## TODO: define your train and validation generator \n",
        "\n",
        "\n",
        "\n",
        "## TODO: train your model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI1D6tyc-HPT"
      },
      "source": [
        "model.evaluate(validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyPP5dAo-vYG"
      },
      "source": [
        "Let's save our model -- we will be using it in the section on convnet visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XSE6zLNG-vYG",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "model.save('cats_and_dogs_small_2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRZY8r1O-vYG"
      },
      "source": [
        "Let's visualize our training using Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LZDuZaW-vYG"
      },
      "source": [
        "%tensorboard --logdir tb_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOLypNoy-vYG"
      },
      "source": [
        "Thanks to data augmentation and dropout, we are no longer overfitting: the training curves are rather closely tracking the validation \n",
        "curves. We are now able to reach an accuracy of 82%, a 15% relative improvement over the non-regularized model.\n",
        "\n",
        "By leveraging regularization techniques even further and by tuning the network's parameters (such as the number of filters per convolution \n",
        "layer, or the number of layers in the network), we may be able to get an even better accuracy, likely up to 86-87%. However, it would prove \n",
        "very difficult to go any higher just by training our own convnet from scratch, simply because we have so little data to work with. As a \n",
        "next step to improve our accuracy on this problem, we will have to leverage transfer learning using pre-trained model, which will be the focus of the \n",
        "lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6kjxaztFdbn"
      },
      "source": [
        "## Test with our own image\n",
        "\n",
        "Now we are ready to put our trained model to test! \n",
        "You can upload any cat and dog image from your local computer using the code below.  The upload file will then be pre-processed into image tensor before feeding into our model for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAdoRJxB-ZvC"
      },
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "uploader = widgets.FileUpload(\n",
        "    accept='image/*',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
        "    multiple=False  # True to accept multiple files upload else False\n",
        ")\n",
        "\n",
        "display(uploader)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hhG2hKh_OfH"
      },
      "source": [
        "fn = next(iter(uploader.value))\n",
        "\n",
        "with open(fn, \"w+b\") as file:\n",
        "    file.write(uploader.data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0mLBA5CNnu4"
      },
      "source": [
        "# We need to pre-process our image to the shape expected by our model \n",
        "\n",
        "img = keras.preprocessing.image.load_img(\n",
        "    fn, target_size=(150, 150)\n",
        ")\n",
        "\n",
        "# we convert the image to numpy array\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "# Although we only have single image, however our model expected data in batches (i.e. a 4D tensor)\n",
        "# so we will need to add in the batch axis too\n",
        "img_array = tf.expand_dims(img_array, 0) # add a batch axis\n",
        "\n",
        "# we load the model saved earlier and do the inference \n",
        "model = tf.keras.models.load_model('cats_and_dogs_small_1')\n",
        "predictions = model(img_array)\n",
        "if predictions[0] > 0.5: \n",
        "    print('It is a dog')\n",
        "else:\n",
        "    print('It is a cat')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}